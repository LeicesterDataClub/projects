---
title: "Read US patent data"
output:
  html_document: default
  html_notebook: default
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(xml2)
```

I've saved one file from the US Patent data [at this link](https://bulkdata.uspto.gov/data/patent/grant/redbook/fulltext/2019/)  and the folowing code will get you started with working on it.

[Note, if you han't come across xml before, you might want to look at an introductory tutorial, e.g. https://www.w3schools.com/xml/default.asp , or something more comprehensive: https://www.ibm.com/developerworks/xml/tutorials/xmlintro/xmlintro.html]

After reading the documentation for `xml2`, the obvious thing to try is:

```{r, dont-run-1, eval = FALSE}
p <- read_xml("Data/ipg190115.xml")
```

Unfortunately this gives an error:

<spam style=color:red>Error in doc_parse_file(con, encoding = encoding, as_html = as_html, options = options) : XML declaration allowed only at the start of the document [64]</span>

You'll need to investigate the file by reading it in with `read_lines`(this will take some time):

```{r}
lines <- read_lines("Data/ipg190115.xml")
```

You should find the file is actually the concatentation of a large number (4137) of individual xml files, each starting with an xml declaration. So if you're going to use the xml protocol you'll have to split it up.

The following code chunk does that, then uses `xml2` functions on each separate file, to extract the text in the first <claim-text> node.

```{r}
xml_declaration <- "<?xml version=\"1.0\" encoding=\"UTF-8\"?>"
# Find which lines have an xml declaration
start_patent <- 
  lines %>% 
  str_which(fixed(xml_declaration))
# Find the last line in each xml section.
# Note the use of the default parameter to deal with the edge case.
stop_patent <- lead(start_patent, 1, default =length(lines) + 1) - 1
# Set up a vector to hold one claim for each separate patent in our file.
claims <- character(length(start_patent))
# For each patent, read it in as xml and finde the first <claim-text> node.
for (p in seq.int(length(start_patent))) {
  pat <- paste(lines[start_patent[p]:(stop_patent[p])], collapse = "")
  patx <- read_xml(pat)
  claims[p] <- xml_text(xml_find_first(patx, ".//claim-text"))
}
```

You now have a large vector containing the text of the first claim in each patent. There are rather a lot of them. We can look at the first six, and then a random sample:

```{r}
set.seed(123)
head(claims)
sample(claims, 10)
```


## Challenge 1

If you just want the text from all the <claim-text> nodes you could use a single regular expression. What is it? (Hint: the answer isn't "chocolate".)

```{r}
search_pattern <- "chocolate"
x <- lines %>%
  str_extract_all(search_pattern) %>% 
  unlist()
sample(x,10)
```

## Challenge 2

Extract a unique identifier for each claim and then create a data frame with two columns: the claim ID, and the claim text.

Then you can use the code from the "Romeo & Juliet" exercise to investigate word frequencies.

## Challenge 3

This data is rather big to store and manipulate in memory - especially if you want to download load many of the files.

Investigate storing the data in an SQL database and sending queries that will operate on the database server rather than having to load everything into memory. Hint: MonetDBLite and RSQLite are packages that avoid you having to install and run a separate database server on your machine.

## Challenge 4

Some of the text contains HTML or XML tags. Can you remove them?




